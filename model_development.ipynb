{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd279a86-b59d-481d-9632-cfa867b22ae2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 30 candidates, totalling 90 fits\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=5, min_samples_split=5, n_estimators=100; total time=   2.1s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=5, min_samples_split=5, n_estimators=100; total time=   2.0s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=5, min_samples_split=5, n_estimators=100; total time=   1.8s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=10, min_samples_split=10, n_estimators=100; total time=   1.6s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=10, min_samples_split=10, n_estimators=100; total time=   1.6s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=10, min_samples_split=10, n_estimators=100; total time=   1.5s\n",
      "[CV] END max_depth=15, max_features=log2, min_samples_leaf=10, min_samples_split=5, n_estimators=100; total time=   1.4s\n",
      "[CV] END max_depth=15, max_features=log2, min_samples_leaf=10, min_samples_split=5, n_estimators=100; total time=   1.5s\n",
      "[CV] END max_depth=15, max_features=log2, min_samples_leaf=10, min_samples_split=5, n_estimators=100; total time=   1.4s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=10, min_samples_split=10, n_estimators=100; total time=   1.9s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=10, min_samples_split=10, n_estimators=100; total time=   2.2s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=10, min_samples_split=10, n_estimators=100; total time=   2.0s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=   1.6s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=   1.5s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=   1.5s\n",
      "[CV] END max_depth=15, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   1.8s\n",
      "[CV] END max_depth=15, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   1.7s\n",
      "[CV] END max_depth=15, max_features=log2, min_samples_leaf=1, min_samples_split=5, n_estimators=100; total time=   1.7s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=300; total time=   6.4s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=300; total time=   6.4s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=1, min_samples_split=2, n_estimators=300; total time=   9.3s\n",
      "[CV] END max_depth=15, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   3.9s\n",
      "[CV] END max_depth=15, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   3.6s\n",
      "[CV] END max_depth=15, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=200; total time=   3.6s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=5, min_samples_split=10, n_estimators=200; total time=   2.8s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=5, min_samples_split=10, n_estimators=200; total time=16.3min\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=5, min_samples_split=10, n_estimators=200; total time=   3.7s\n",
      "[CV] END max_depth=15, max_features=log2, min_samples_leaf=10, min_samples_split=5, n_estimators=200; total time=   2.6s\n",
      "[CV] END max_depth=15, max_features=log2, min_samples_leaf=10, min_samples_split=5, n_estimators=200; total time= 7.1min\n",
      "[CV] END max_depth=15, max_features=log2, min_samples_leaf=10, min_samples_split=5, n_estimators=200; total time=   5.5s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=   3.4s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=   2.9s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=100; total time=   3.1s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=10, min_samples_split=5, n_estimators=100; total time=   1.8s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=10, min_samples_split=5, n_estimators=100; total time=   2.0s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=10, min_samples_split=5, n_estimators=100; total time=   1.8s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   4.7s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   5.1s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   5.8s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   1.6s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   1.8s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   1.6s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=10, min_samples_split=5, n_estimators=200; total time=   2.4s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=10, min_samples_split=5, n_estimators=200; total time=   2.5s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=10, min_samples_split=5, n_estimators=200; total time=   2.3s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=10, min_samples_split=5, n_estimators=300; total time=   5.3s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=10, min_samples_split=5, n_estimators=300; total time=   6.3s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=10, min_samples_split=5, n_estimators=300; total time=   5.9s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=10, min_samples_split=2, n_estimators=200; total time=   3.4s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=10, min_samples_split=2, n_estimators=200; total time=   3.6s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=10, min_samples_split=2, n_estimators=200; total time=   3.6s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=5, min_samples_split=2, n_estimators=100; total time=   1.2s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=5, min_samples_split=2, n_estimators=100; total time=   1.2s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=5, min_samples_split=2, n_estimators=100; total time=   1.3s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   4.3s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   4.2s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=2, n_estimators=200; total time=   4.2s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=10, min_samples_split=5, n_estimators=100; total time=   1.0s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=10, min_samples_split=5, n_estimators=100; total time=   1.0s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=10, min_samples_split=5, n_estimators=100; total time=   1.0s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=   1.6s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=   1.6s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=1, min_samples_split=10, n_estimators=100; total time=   1.8s\n",
      "[CV] END max_depth=15, max_features=log2, min_samples_leaf=10, min_samples_split=10, n_estimators=200; total time=   2.5s\n",
      "[CV] END max_depth=15, max_features=log2, min_samples_leaf=10, min_samples_split=10, n_estimators=200; total time=   2.5s\n",
      "[CV] END max_depth=15, max_features=log2, min_samples_leaf=10, min_samples_split=10, n_estimators=200; total time=   2.4s\n",
      "[CV] END max_depth=15, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=   3.5s\n",
      "[CV] END max_depth=15, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=   4.0s\n",
      "[CV] END max_depth=15, max_features=sqrt, min_samples_leaf=1, min_samples_split=10, n_estimators=200; total time=   4.3s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=5, min_samples_split=10, n_estimators=200; total time=   2.9s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=5, min_samples_split=10, n_estimators=200; total time=   3.1s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=5, min_samples_split=10, n_estimators=200; total time=   2.6s\n",
      "[CV] END max_depth=15, max_features=sqrt, min_samples_leaf=10, min_samples_split=5, n_estimators=300; total time=   4.6s\n",
      "[CV] END max_depth=15, max_features=sqrt, min_samples_leaf=10, min_samples_split=5, n_estimators=300; total time=   4.9s\n",
      "[CV] END max_depth=15, max_features=sqrt, min_samples_leaf=10, min_samples_split=5, n_estimators=300; total time=   4.6s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=300; total time=   7.4s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=300; total time=   7.3s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=1, min_samples_split=2, n_estimators=300; total time=   7.1s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=300; total time=   6.7s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=300; total time=   5.9s\n",
      "[CV] END max_depth=20, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=300; total time=   6.0s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=   2.2s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=   2.6s\n",
      "[CV] END max_depth=10, max_features=sqrt, min_samples_leaf=2, min_samples_split=10, n_estimators=200; total time=   2.5s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   1.0s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   1.0s\n",
      "[CV] END max_depth=10, max_features=log2, min_samples_leaf=2, min_samples_split=2, n_estimators=100; total time=   1.0s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=5, min_samples_split=2, n_estimators=200; total time=   2.7s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=5, min_samples_split=2, n_estimators=200; total time=   3.0s\n",
      "[CV] END max_depth=20, max_features=log2, min_samples_leaf=5, min_samples_split=2, n_estimators=200; total time=   2.7s\n",
      "\n",
      "Best Hyperparameters: {'n_estimators': 300, 'min_samples_split': 2, 'min_samples_leaf': 1, 'max_features': 'log2', 'max_depth': 20}\n",
      "Best Average MAE on Validation Folds: -0.5792\n",
      "Test Set MAE: 0.4851\n",
      "Feature Importances:\n",
      "gamma_call                     0.070619\n",
      "gamma_put                      0.070557\n",
      "theta_put                      0.056898\n",
      "theta_call                     0.053390\n",
      "vol_call                       0.040340\n",
      "iv_current                     0.039825\n",
      "vol_put                        0.039582\n",
      "iv_week_ago                    0.039180\n",
      "hv_week_ago                    0.037986\n",
      "hv_current                     0.036543\n",
      "hv_year_low_date_length_ago    0.035553\n",
      "vega_put                       0.033663\n",
      "vega_call                      0.033652\n",
      "iv_year_low_date_length_ago    0.033475\n",
      "hv_year_high_length_ago        0.033435\n",
      "iv_year_high_length_ago        0.031162\n",
      "iv_year_high                   0.027962\n",
      "relative_spread_call           0.027786\n",
      "relative_spread_put            0.027741\n",
      "hv_year_high                   0.026148\n",
      "rho_put                        0.025406\n",
      "rho_call                       0.025342\n",
      "mid_put                        0.025323\n",
      "relative_strike                0.023816\n",
      "delta_call                     0.023710\n",
      "mid_call                       0.023150\n",
      "delta_put                      0.021865\n",
      "hv_year_low                    0.014862\n",
      "iv_year_low                    0.014324\n",
      "4_week_option                  0.006706\n",
      "dtype: float64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split, KFold, RandomizedSearchCV\n",
    "from sklearn.metrics import mean_absolute_error, make_scorer\n",
    "import numpy as np\n",
    "\n",
    "# Load the data\n",
    "file_path = 'final_straddle_data_normalized.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Remove the month columns\n",
    "month_columns = [col for col in df.columns if 'month' in col.lower()]\n",
    "df = df.drop(columns=month_columns)\n",
    "\n",
    "# Define features and target\n",
    "features = df.drop(columns=['date', 'delta_neutral_long_straddle_returns', 'mean_daily_delta_neutral_long_straddle_returns'])\n",
    "target = df['mean_daily_delta_neutral_long_straddle_returns']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the parameter grid for hyperparameter tuning\n",
    "param_dist = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "    'max_depth': [10, 15, 20],\n",
    "    'min_samples_split': [2, 5, 10],\n",
    "    'min_samples_leaf': [1, 2, 5, 10]\n",
    "}\n",
    "\n",
    "# Define MAE as the scoring metric\n",
    "mae_scorer = make_scorer(mean_absolute_error, greater_is_better=False)\n",
    "\n",
    "# Initialize the Random Forest model\n",
    "rf = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# Initialize RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(estimator=rf, param_distributions=param_dist, n_iter=30, scoring=mae_scorer, cv=3, random_state=42, n_jobs=1, verbose=2)\n",
    "\n",
    "# Perform random search\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters and the corresponding MAE\n",
    "best_params = random_search.best_params_\n",
    "best_score = random_search.best_score_\n",
    "\n",
    "print(f\"\\nBest Hyperparameters: {best_params}\")\n",
    "print(f\"Best Average MAE on Validation Folds: {best_score:.4f}\")\n",
    "\n",
    "# Train the best model on the entire training set\n",
    "best_rf = random_search.best_estimator_\n",
    "best_rf.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = best_rf.predict(X_test)\n",
    "\n",
    "# Calculate and print the MAE on the test set\n",
    "test_mae = mean_absolute_error(y_test, y_pred)\n",
    "print(f\"Test Set MAE: {test_mae:.4f}\")\n",
    "\n",
    "# Print feature importances\n",
    "feature_importances = pd.Series(best_rf.feature_importances_, index=features.columns).sort_values(ascending=False)\n",
    "print('Feature Importances:')\n",
    "print(feature_importances)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd0fa86-b66c-49b4-8d58-356e1755555b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0f2717d7-bcbd-4288-b414-753356c38198",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params: {'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 5, 'min_samples_split': 5, 'n_estimators': 100}\n",
      "  Mean MAE on Train Folds: 0.5415 (std: 0.0069)\n",
      "  Mean MAE on Validation Folds: 0.6518 (std: 0.0191)\n",
      "Params: {'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 5, 'min_samples_split': 5, 'n_estimators': 200}\n",
      "  Mean MAE on Train Folds: 0.5443 (std: 0.0051)\n",
      "  Mean MAE on Validation Folds: 0.6561 (std: 0.0205)\n",
      "Params: {'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 5, 'min_samples_split': 5, 'n_estimators': 300}\n",
      "  Mean MAE on Train Folds: 0.5410 (std: 0.0070)\n",
      "  Mean MAE on Validation Folds: 0.6532 (std: 0.0200)\n",
      "Params: {'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 5, 'min_samples_split': 10, 'n_estimators': 100}\n",
      "  Mean MAE on Train Folds: 0.5408 (std: 0.0071)\n",
      "  Mean MAE on Validation Folds: 0.6514 (std: 0.0229)\n",
      "Params: {'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 5, 'min_samples_split': 10, 'n_estimators': 200}\n",
      "  Mean MAE on Train Folds: 0.5452 (std: 0.0063)\n",
      "  Mean MAE on Validation Folds: 0.6547 (std: 0.0210)\n",
      "Params: {'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 5, 'min_samples_split': 10, 'n_estimators': 300}\n",
      "  Mean MAE on Train Folds: 0.5417 (std: 0.0098)\n",
      "  Mean MAE on Validation Folds: 0.6539 (std: 0.0180)\n",
      "Params: {'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 10, 'min_samples_split': 5, 'n_estimators': 100}\n",
      "  Mean MAE on Train Folds: 0.5859 (std: 0.0081)\n",
      "  Mean MAE on Validation Folds: 0.6707 (std: 0.0163)\n",
      "Params: {'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 10, 'min_samples_split': 5, 'n_estimators': 200}\n",
      "  Mean MAE on Train Folds: 0.5850 (std: 0.0061)\n",
      "  Mean MAE on Validation Folds: 0.6687 (std: 0.0188)\n",
      "Params: {'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 10, 'min_samples_split': 5, 'n_estimators': 300}\n",
      "  Mean MAE on Train Folds: 0.5846 (std: 0.0062)\n",
      "  Mean MAE on Validation Folds: 0.6691 (std: 0.0201)\n",
      "Params: {'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 10, 'min_samples_split': 10, 'n_estimators': 100}\n",
      "  Mean MAE on Train Folds: 0.5868 (std: 0.0100)\n",
      "  Mean MAE on Validation Folds: 0.6683 (std: 0.0159)\n",
      "Params: {'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 10, 'min_samples_split': 10, 'n_estimators': 200}\n",
      "  Mean MAE on Train Folds: 0.5838 (std: 0.0070)\n",
      "  Mean MAE on Validation Folds: 0.6676 (std: 0.0180)\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 45\u001b[0m\n\u001b[1;32m     42\u001b[0m y_tr, y_val \u001b[38;5;241m=\u001b[39m y_train\u001b[38;5;241m.\u001b[39miloc[train_index], y_train\u001b[38;5;241m.\u001b[39miloc[val_index]\n\u001b[1;32m     44\u001b[0m rf \u001b[38;5;241m=\u001b[39m RandomForestRegressor(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[0;32m---> 45\u001b[0m \u001b[43mrf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_tr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_tr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     47\u001b[0m y_tr_pred \u001b[38;5;241m=\u001b[39m rf\u001b[38;5;241m.\u001b[39mpredict(X_tr)\n\u001b[1;32m     48\u001b[0m y_val_pred \u001b[38;5;241m=\u001b[39m rf\u001b[38;5;241m.\u001b[39mpredict(X_val)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1472\u001b[0m     )\n\u001b[1;32m   1473\u001b[0m ):\n\u001b[0;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_forest.py:489\u001b[0m, in \u001b[0;36mBaseForest.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m    478\u001b[0m trees \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    479\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_estimator(append\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39mrandom_state)\n\u001b[1;32m    480\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_more_estimators)\n\u001b[1;32m    481\u001b[0m ]\n\u001b[1;32m    483\u001b[0m \u001b[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[1;32m    484\u001b[0m \u001b[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[1;32m    485\u001b[0m \u001b[38;5;66;03m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[1;32m    486\u001b[0m \u001b[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[1;32m    487\u001b[0m \u001b[38;5;66;03m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[1;32m    488\u001b[0m \u001b[38;5;66;03m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[0;32m--> 489\u001b[0m trees \u001b[38;5;241m=\u001b[39m \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    490\u001b[0m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    491\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    492\u001b[0m \u001b[43m    \u001b[49m\u001b[43mprefer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mthreads\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_parallel_build_trees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    495\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    496\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    497\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    498\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    499\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    500\u001b[0m \u001b[43m        \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    501\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    502\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    503\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    504\u001b[0m \u001b[43m        \u001b[49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    505\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    506\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    507\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    508\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    510\u001b[0m \u001b[38;5;66;03m# Collect newly grown trees\u001b[39;00m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mextend(trees)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/utils/parallel.py:67\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m     62\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[1;32m     63\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m     64\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[1;32m     66\u001b[0m )\n\u001b[0;32m---> 67\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/joblib/parallel.py:1863\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1861\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_sequential_output(iterable)\n\u001b[1;32m   1862\u001b[0m     \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[0;32m-> 1863\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturn_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(output)\n\u001b[1;32m   1865\u001b[0m \u001b[38;5;66;03m# Let's create an ID that uniquely identifies the current call. If the\u001b[39;00m\n\u001b[1;32m   1866\u001b[0m \u001b[38;5;66;03m# call is interrupted early and that the same instance is immediately\u001b[39;00m\n\u001b[1;32m   1867\u001b[0m \u001b[38;5;66;03m# re-used, this id will be used to prevent workers that were\u001b[39;00m\n\u001b[1;32m   1868\u001b[0m \u001b[38;5;66;03m# concurrently finalizing a task from the previous call to run the\u001b[39;00m\n\u001b[1;32m   1869\u001b[0m \u001b[38;5;66;03m# callback.\u001b[39;00m\n\u001b[1;32m   1870\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/joblib/parallel.py:1792\u001b[0m, in \u001b[0;36mParallel._get_sequential_output\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m   1790\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_batches \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1791\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_dispatched_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m-> 1792\u001b[0m res \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1793\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_completed_tasks \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1794\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mprint_progress()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/utils/parallel.py:129\u001b[0m, in \u001b[0;36m_FuncWrapper.__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    127\u001b[0m     config \u001b[38;5;241m=\u001b[39m {}\n\u001b[1;32m    128\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mconfig):\n\u001b[0;32m--> 129\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/ensemble/_forest.py:192\u001b[0m, in \u001b[0;36m_parallel_build_trees\u001b[0;34m(tree, bootstrap, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight, n_samples_bootstrap, missing_values_in_feature_mask)\u001b[0m\n\u001b[1;32m    189\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m class_weight \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced_subsample\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    190\u001b[0m         curr_sample_weight \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m=\u001b[39m compute_sample_weight(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbalanced\u001b[39m\u001b[38;5;124m\"\u001b[39m, y, indices\u001b[38;5;241m=\u001b[39mindices)\n\u001b[0;32m--> 192\u001b[0m     \u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    193\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    195\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurr_sample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    196\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    197\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    200\u001b[0m     tree\u001b[38;5;241m.\u001b[39m_fit(\n\u001b[1;32m    201\u001b[0m         X,\n\u001b[1;32m    202\u001b[0m         y,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    205\u001b[0m         missing_values_in_feature_mask\u001b[38;5;241m=\u001b[39mmissing_values_in_feature_mask,\n\u001b[1;32m    206\u001b[0m     )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/tree/_classes.py:472\u001b[0m, in \u001b[0;36mBaseDecisionTree._fit\u001b[0;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    462\u001b[0m     builder \u001b[38;5;241m=\u001b[39m BestFirstTreeBuilder(\n\u001b[1;32m    463\u001b[0m         splitter,\n\u001b[1;32m    464\u001b[0m         min_samples_split,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    469\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_impurity_decrease,\n\u001b[1;32m    470\u001b[0m     )\n\u001b[0;32m--> 472\u001b[0m \u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    474\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    475\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split, KFold, ParameterGrid\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import numpy as np\n",
    "\n",
    "# Load the data\n",
    "file_path = 'final_straddle_data_normalized.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Define features and target\n",
    "features = df.drop(columns=['date', 'delta_neutral_long_straddle_returns', 'mean_daily_delta_neutral_long_straddle_returns'])\n",
    "target = df['mean_daily_delta_neutral_long_straddle_returns']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the expanded parameter grid for hyperparameter tuning\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "    'max_depth': [10, 15],\n",
    "    'min_samples_split': [5, 10],\n",
    "    'min_samples_leaf': [5, 10]\n",
    "}\n",
    "\n",
    "# Define MAE as the scoring metric\n",
    "def mae_scorer(y_true, y_pred):\n",
    "    return mean_absolute_error(y_true, y_pred)\n",
    "\n",
    "# Custom cross-validation loop\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "best_params = None\n",
    "best_score = float('inf')\n",
    "\n",
    "for params in ParameterGrid(param_grid):\n",
    "    train_scores = []\n",
    "    val_scores = []\n",
    "    \n",
    "    for train_index, val_index in kf.split(X_train):\n",
    "        X_tr, X_val = X_train.iloc[train_index], X_train.iloc[val_index]\n",
    "        y_tr, y_val = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "        \n",
    "        rf = RandomForestRegressor(**params)\n",
    "        rf.fit(X_tr, y_tr)\n",
    "        \n",
    "        y_tr_pred = rf.predict(X_tr)\n",
    "        y_val_pred = rf.predict(X_val)\n",
    "        \n",
    "        train_mae = mae_scorer(y_tr, y_tr_pred)\n",
    "        val_mae = mae_scorer(y_val, y_val_pred)\n",
    "        \n",
    "        train_scores.append(train_mae)\n",
    "        val_scores.append(val_mae)\n",
    "    \n",
    "    mean_train_mae = np.mean(train_scores)\n",
    "    std_train_mae = np.std(train_scores)\n",
    "    mean_val_mae = np.mean(val_scores)\n",
    "    std_val_mae = np.std(val_scores)\n",
    "    \n",
    "    print(f\"Params: {params}\")\n",
    "    print(f\"  Mean MAE on Train Folds: {mean_train_mae:.4f} (std: {std_train_mae:.4f})\")\n",
    "    print(f\"  Mean MAE on Validation Folds: {mean_val_mae:.4f} (std: {std_val_mae:.4f})\")\n",
    "    \n",
    "    if mean_val_mae < best_score:\n",
    "        best_score = mean_val_mae\n",
    "        best_params = params\n",
    "\n",
    "print(f\"\\nBest Hyperparameters: {best_params}\")\n",
    "print(f\"Best Average MAE on Validation Folds: {best_score:.4f}\")\n",
    "\n",
    "# Train the best model on the entire training set\n",
    "best_rf = RandomForestRegressor(**best_params)\n",
    "best_rf.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = best_rf.predict(X_test)\n",
    "\n",
    "# Calculate and print the MAE on the test set\n",
    "test_mae = mae_scorer(y_test, y_pred)\n",
    "print(f\"Test Set MAE: {test_mae:.4f}\")\n",
    "\n",
    "# Print feature importances\n",
    "feature_importances = pd.Series(best_rf.feature_importances_, index=features.columns).sort_values(ascending=False)\n",
    "print('Feature Importances:')\n",
    "print(feature_importances)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "de6f4bc9-680b-4eec-b121-74062c43572f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params: {'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 5, 'min_samples_split': 5, 'n_estimators': 100}\n",
      "  Mean MAE on Train Folds: 0.5458 (std: 0.0063)\n",
      "  Mean MAE on Validation Folds: 0.6570 (std: 0.0212)\n",
      "Params: {'max_depth': 10, 'max_features': 'sqrt', 'min_samples_leaf': 5, 'min_samples_split': 5, 'n_estimators': 200}\n",
      "  Mean MAE on Train Folds: 0.5427 (std: 0.0060)\n",
      "  Mean MAE on Validation Folds: 0.6516 (std: 0.0226)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split, KFold, ParameterGrid\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import numpy as np\n",
    "\n",
    "# Load the data\n",
    "file_path = 'final_straddle_data_normalized.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Define features and target\n",
    "features = df.drop(columns=['date', 'delta_neutral_long_straddle_returns', 'mean_daily_delta_neutral_long_straddle_returns'])\n",
    "target = df['mean_daily_delta_neutral_long_straddle_returns']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the expanded parameter grid for hyperparameter tuning\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'max_features': ['sqrt', 'log2'],\n",
    "    'max_depth': [10, 15],\n",
    "    'min_samples_split': [5, 10],\n",
    "    'min_samples_leaf': [5, 10]\n",
    "}\n",
    "\n",
    "# Define MAE as the scoring metric\n",
    "def mae_scorer(y_true, y_pred):\n",
    "    return mean_absolute_error(y_true, y_pred)\n",
    "\n",
    "# Custom cross-validation loop\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "best_params = None\n",
    "best_score = float('inf')\n",
    "\n",
    "for params in ParameterGrid(param_grid):\n",
    "    train_scores = []\n",
    "    val_scores = []\n",
    "    \n",
    "    for train_index, val_index in kf.split(X_train):\n",
    "        X_tr, X_val = X_train.iloc[train_index], X_train.iloc[val_index]\n",
    "        y_tr, y_val = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "        \n",
    "        rf = RandomForestRegressor(**params)\n",
    "        rf.fit(X_tr, y_tr)\n",
    "        \n",
    "        y_tr_pred = rf.predict(X_tr)\n",
    "        y_val_pred = rf.predict(X_val)\n",
    "        \n",
    "        train_mae = mae_scorer(y_tr, y_tr_pred)\n",
    "        val_mae = mae_scorer(y_val, y_val_pred)\n",
    "        \n",
    "        train_scores.append(train_mae)\n",
    "        val_scores.append(val_mae)\n",
    "    \n",
    "    mean_train_mae = np.mean(train_scores)\n",
    "    std_train_mae = np.std(train_scores)\n",
    "    mean_val_mae = np.mean(val_scores)\n",
    "    std_val_mae = np.std(val_scores)\n",
    "    \n",
    "    print(f\"Params: {params}\")\n",
    "    print(f\"  Mean MAE on Train Folds: {mean_train_mae:.4f} (std: {std_train_mae:.4f})\")\n",
    "    print(f\"  Mean MAE on Validation Folds: {mean_val_mae:.4f} (std: {std_val_mae:.4f})\")\n",
    "    \n",
    "    if mean_val_mae < best_score:\n",
    "        best_score = mean_val_mae\n",
    "        best_params = params\n",
    "\n",
    "print(f\"\\nBest Hyperparameters: {best_params}\")\n",
    "print(f\"Best Average MAE on Validation Folds: {best_score:.4f}\")\n",
    "\n",
    "# Train the best model on the entire training set\n",
    "best_rf = RandomForestRegressor(**best_params)\n",
    "best_rf.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = best_rf.predict(X_test)\n",
    "\n",
    "# Calculate and print the MAE on the test set\n",
    "test_mae = mae_scorer(y_test, y_pred)\n",
    "print(f\"Test Set MAE: {test_mae:.4f}\")\n",
    "\n",
    "# Print feature importances\n",
    "feature_importances = pd.Series(best_rf.feature_importances_, index=features.columns).sort_values(ascending=False)\n",
    "print('Feature Importances:')\n",
    "print(feature_importances)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "155455c3-a935-47dc-87ea-7a68defa5c1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Params: {'learning_rate': 0.01, 'max_depth': 3, 'min_samples_leaf': 5, 'min_samples_split': 5, 'n_estimators': 100}\n",
      "  Mean RMSE on Train Folds: 1.5728 (std: 0.0198)\n",
      "  Mean RMSE on Validation Folds: 1.6281 (std: 0.0785)\n",
      "Params: {'learning_rate': 0.01, 'max_depth': 3, 'min_samples_leaf': 5, 'min_samples_split': 5, 'n_estimators': 200}\n",
      "  Mean RMSE on Train Folds: 1.4984 (std: 0.0205)\n",
      "  Mean RMSE on Validation Folds: 1.5943 (std: 0.0686)\n",
      "Params: {'learning_rate': 0.01, 'max_depth': 3, 'min_samples_leaf': 5, 'min_samples_split': 5, 'n_estimators': 300}\n",
      "  Mean RMSE on Train Folds: 1.4361 (std: 0.0176)\n",
      "  Mean RMSE on Validation Folds: 1.5613 (std: 0.0598)\n",
      "Params: {'learning_rate': 0.01, 'max_depth': 3, 'min_samples_leaf': 5, 'min_samples_split': 10, 'n_estimators': 100}\n",
      "  Mean RMSE on Train Folds: 1.5728 (std: 0.0198)\n",
      "  Mean RMSE on Validation Folds: 1.6281 (std: 0.0785)\n",
      "Params: {'learning_rate': 0.01, 'max_depth': 3, 'min_samples_leaf': 5, 'min_samples_split': 10, 'n_estimators': 200}\n",
      "  Mean RMSE on Train Folds: 1.4984 (std: 0.0205)\n",
      "  Mean RMSE on Validation Folds: 1.5943 (std: 0.0686)\n",
      "Params: {'learning_rate': 0.01, 'max_depth': 3, 'min_samples_leaf': 5, 'min_samples_split': 10, 'n_estimators': 300}\n",
      "  Mean RMSE on Train Folds: 1.4361 (std: 0.0176)\n",
      "  Mean RMSE on Validation Folds: 1.5613 (std: 0.0598)\n",
      "Params: {'learning_rate': 0.01, 'max_depth': 3, 'min_samples_leaf': 5, 'min_samples_split': 20, 'n_estimators': 100}\n",
      "  Mean RMSE on Train Folds: 1.5770 (std: 0.0230)\n",
      "  Mean RMSE on Validation Folds: 1.6299 (std: 0.0774)\n",
      "Params: {'learning_rate': 0.01, 'max_depth': 3, 'min_samples_leaf': 5, 'min_samples_split': 20, 'n_estimators': 200}\n",
      "  Mean RMSE on Train Folds: 1.5060 (std: 0.0223)\n",
      "  Mean RMSE on Validation Folds: 1.5969 (std: 0.0674)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x1117d5a90>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthewbellick/anaconda3/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 770, in _clean_thread_parent_frames\n",
      "    def _clean_thread_parent_frames(\n",
      "\n",
      "KeyboardInterrupt: \n",
      "Exception ignored in: <bound method IPythonKernel._clean_thread_parent_frames of <ipykernel.ipkernel.IPythonKernel object at 0x1117d5a90>>\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/matthewbellick/anaconda3/lib/python3.11/site-packages/ipykernel/ipkernel.py\", line 770, in _clean_thread_parent_frames\n",
      "KeyboardInterrupt: \n",
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split, KFold, ParameterGrid\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "# Load the data\n",
    "file_path = 'final_straddle_data_normalized.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Define features and target\n",
    "features = df.drop(columns=['date', 'delta_neutral_long_straddle_returns', 'mean_daily_delta_neutral_long_straddle_returns'])\n",
    "target = df['mean_daily_delta_neutral_long_straddle_returns']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the parameter grid for hyperparameter tuning\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'min_samples_split': [5, 10, 20],\n",
    "    'min_samples_leaf': [5, 10, 20]\n",
    "}\n",
    "\n",
    "# Define RMSE as the scoring metric\n",
    "def rmse_scorer(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "# Custom cross-validation loop\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "best_params = None\n",
    "best_score = float('inf')\n",
    "\n",
    "for params in ParameterGrid(param_grid):\n",
    "    train_scores = []\n",
    "    val_scores = []\n",
    "    \n",
    "    for train_index, val_index in kf.split(X_train):\n",
    "        X_tr, X_val = X_train.iloc[train_index], X_train.iloc[val_index]\n",
    "        y_tr, y_val = y_train.iloc[train_index], y_train.iloc[val_index]\n",
    "        \n",
    "        gbr = GradientBoostingRegressor(**params, random_state=42)\n",
    "        gbr.fit(X_tr, y_tr)\n",
    "        \n",
    "        y_tr_pred = gbr.predict(X_tr)\n",
    "        y_val_pred = gbr.predict(X_val)\n",
    "        \n",
    "        train_rmse = rmse_scorer(y_tr, y_tr_pred)\n",
    "        val_rmse = rmse_scorer(y_val, y_val_pred)\n",
    "        \n",
    "        train_scores.append(train_rmse)\n",
    "        val_scores.append(val_rmse)\n",
    "    \n",
    "    mean_train_rmse = np.mean(train_scores)\n",
    "    std_train_rmse = np.std(train_scores)\n",
    "    mean_val_rmse = np.mean(val_scores)\n",
    "    std_val_rmse = np.std(val_scores)\n",
    "    \n",
    "    print(f\"Params: {params}\")\n",
    "    print(f\"  Mean RMSE on Train Folds: {mean_train_rmse:.4f} (std: {std_train_rmse:.4f})\")\n",
    "    print(f\"  Mean RMSE on Validation Folds: {mean_val_rmse:.4f} (std: {std_val_rmse:.4f})\")\n",
    "    \n",
    "    if mean_val_rmse < best_score:\n",
    "        best_score = mean_val_rmse\n",
    "        best_params = params\n",
    "\n",
    "print(f\"\\nBest Hyperparameters: {best_params}\")\n",
    "print(f\"Best Average RMSE on Validation Folds: {best_score:.4f}\")\n",
    "\n",
    "# Train the best model on the entire training set\n",
    "best_gbr = GradientBoostingRegressor(**best_params, random_state=42)\n",
    "best_gbr.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = best_gbr.predict(X_test)\n",
    "\n",
    "# Calculate and print the RMSE on the test set\n",
    "test_rmse = rmse_scorer(y_test, y_pred)\n",
    "print(f\"Test Set RMSE: {test_rmse}\")\n",
    "\n",
    "# Print feature importances\n",
    "feature_importances = pd.Series(best_gbr.feature_importances_, index=features.columns).sort_values(ascending=False)\n",
    "print('Feature Importances:')\n",
    "print(feature_importances)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8316775-23ae-437b-a853-54b810595cd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.model_selection import train_test_split, KFold, RandomizedSearchCV\n",
    "from sklearn.metrics import mean_squared_error, make_scorer\n",
    "import numpy as np\n",
    "\n",
    "# Load the data\n",
    "file_path = 'final_straddle_data_normalized.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "\n",
    "# Define features and target\n",
    "features = df.drop(columns=['date', 'delta_neutral_long_straddle_returns', 'mean_daily_delta_neutral_long_straddle_returns'])\n",
    "target = df['mean_daily_delta_neutral_long_straddle_returns']\n",
    "\n",
    "# Split the data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, target, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define the parameter grid for hyperparameter tuning\n",
    "param_dist = {\n",
    "    'n_estimators': [200, 500, 1000],\n",
    "    'learning_rate': [0.01, 0.05, 0.1],\n",
    "    'max_depth': [3, 5, 7, 10],\n",
    "    'min_samples_split': [5, 10, 20],\n",
    "    'min_samples_leaf': [5, 10, 20],\n",
    "    'subsample': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "# Define RMSE as the scoring metric\n",
    "def rmse_scorer(y_true, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "\n",
    "# Custom scorer\n",
    "scorer = make_scorer(rmse_scorer, greater_is_better=False)\n",
    "\n",
    "# Initialize the RandomizedSearchCV with 20 iterations\n",
    "gbr = GradientBoostingRegressor(random_state=42)\n",
    "random_search = RandomizedSearchCV(estimator=gbr, param_distributions=param_dist, n_iter=30, scoring=scorer, cv=5, random_state=42, n_jobs=-1, verbose=1)\n",
    "\n",
    "# Perform random search\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best parameters and the corresponding RMSE\n",
    "best_params = random_search.best_params_\n",
    "best_score = -random_search.best_score_\n",
    "\n",
    "print(f\"\\nBest Hyperparameters: {best_params}\")\n",
    "print(f\"Best Average RMSE on Validation Folds: {best_score:.4f}\")\n",
    "\n",
    "# Train the best model on the entire training set\n",
    "best_gbr = random_search.best_estimator_\n",
    "best_gbr.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = best_gbr.predict(X_test)\n",
    "\n",
    "# Calculate and print the RMSE on the test set\n",
    "test_rmse = rmse_scorer(y_test, y_pred)\n",
    "print(f\"Test Set RMSE: {test_rmse:.4f}\")\n",
    "\n",
    "# Print feature importances\n",
    "feature_importances = pd.Series(best_gbr.feature_importances_, index=features.columns).sort_values(ascending=False)\n",
    "print('Feature Importances:')\n",
    "print(feature_importances)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96b3f2e-bb54-4e37-a326-f0bb033f6a78",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac82171e-6985-4770-94c9-dffcb3efe3df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4254fed-c729-4ede-8190-6474a4c0feb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4fdee11-2982-4aa1-a541-be2e1c98ef8e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f2cdf12-37fa-4f09-89ab-31218a5c4b4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46086edd-03ca-4685-8183-bf151f4cec00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f4f0ab3f-8443-4ad5-a211-f9c7e15d0662",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hyperparameters: {'weights': 'uniform', 'p': 1, 'n_neighbors': 12}\n",
      "Mean Train MAE: 0.5088\n",
      "Mean Test MAE: 0.8918\n",
      "\n",
      "Hyperparameters: {'weights': 'distance', 'p': 2, 'n_neighbors': 12}\n",
      "Mean Train MAE: 0.0000\n",
      "Mean Test MAE: 0.9209\n",
      "\n",
      "Hyperparameters: {'weights': 'uniform', 'p': 1, 'n_neighbors': 2}\n",
      "Mean Train MAE: 0.2339\n",
      "Mean Test MAE: 1.0259\n",
      "\n",
      "Hyperparameters: {'weights': 'distance', 'p': 2, 'n_neighbors': 14}\n",
      "Mean Train MAE: 0.0000\n",
      "Mean Test MAE: 0.9111\n",
      "\n",
      "Hyperparameters: {'weights': 'uniform', 'p': 2, 'n_neighbors': 7}\n",
      "Mean Train MAE: 0.4660\n",
      "Mean Test MAE: 0.9596\n",
      "\n",
      "Hyperparameters: {'weights': 'uniform', 'p': 1, 'n_neighbors': 17}\n",
      "Mean Train MAE: 0.5651\n",
      "Mean Test MAE: 0.8712\n",
      "\n",
      "Hyperparameters: {'weights': 'distance', 'p': 1, 'n_neighbors': 19}\n",
      "Mean Train MAE: -0.0000\n",
      "Mean Test MAE: 0.8702\n",
      "\n",
      "Hyperparameters: {'weights': 'uniform', 'p': 2, 'n_neighbors': 3}\n",
      "Mean Train MAE: 0.3002\n",
      "Mean Test MAE: 1.0350\n",
      "\n",
      "Hyperparameters: {'weights': 'uniform', 'p': 1, 'n_neighbors': 11}\n",
      "Mean Train MAE: 0.4927\n",
      "Mean Test MAE: 0.8984\n",
      "\n",
      "Hyperparameters: {'weights': 'distance', 'p': 2, 'n_neighbors': 27}\n",
      "Mean Train MAE: 0.0000\n",
      "Mean Test MAE: 0.8837\n",
      "\n",
      "Hyperparameters: {'weights': 'uniform', 'p': 2, 'n_neighbors': 5}\n",
      "Mean Train MAE: 0.4014\n",
      "Mean Test MAE: 0.9899\n",
      "\n",
      "Hyperparameters: {'weights': 'uniform', 'p': 2, 'n_neighbors': 16}\n",
      "Mean Train MAE: 0.5943\n",
      "Mean Test MAE: 0.9007\n",
      "\n",
      "Hyperparameters: {'weights': 'distance', 'p': 2, 'n_neighbors': 3}\n",
      "Mean Train MAE: 0.0000\n",
      "Mean Test MAE: 1.0354\n",
      "\n",
      "Hyperparameters: {'weights': 'uniform', 'p': 1, 'n_neighbors': 10}\n",
      "Mean Train MAE: 0.4772\n",
      "Mean Test MAE: 0.9062\n",
      "\n",
      "Hyperparameters: {'weights': 'distance', 'p': 1, 'n_neighbors': 23}\n",
      "Mean Train MAE: -0.0000\n",
      "Mean Test MAE: 0.8629\n",
      "\n",
      "Hyperparameters: {'weights': 'distance', 'p': 2, 'n_neighbors': 23}\n",
      "Mean Train MAE: 0.0000\n",
      "Mean Test MAE: 0.8873\n",
      "\n",
      "Hyperparameters: {'weights': 'distance', 'p': 1, 'n_neighbors': 28}\n",
      "Mean Train MAE: -0.0000\n",
      "Mean Test MAE: 0.8564\n",
      "\n",
      "Hyperparameters: {'weights': 'uniform', 'p': 1, 'n_neighbors': 1}\n",
      "Mean Train MAE: -0.0000\n",
      "Mean Test MAE: 1.1233\n",
      "\n",
      "Hyperparameters: {'weights': 'uniform', 'p': 1, 'n_neighbors': 23}\n",
      "Mean Train MAE: 0.6075\n",
      "Mean Test MAE: 0.8606\n",
      "\n",
      "Hyperparameters: {'weights': 'uniform', 'p': 1, 'n_neighbors': 27}\n",
      "Mean Train MAE: 0.6272\n",
      "Mean Test MAE: 0.8553\n",
      "\n",
      "Hyperparameters: {'weights': 'distance', 'p': 1, 'n_neighbors': 17}\n",
      "Mean Train MAE: -0.0000\n",
      "Mean Test MAE: 0.8747\n",
      "\n",
      "Hyperparameters: {'weights': 'distance', 'p': 1, 'n_neighbors': 12}\n",
      "Mean Train MAE: -0.0000\n",
      "Mean Test MAE: 0.8948\n",
      "\n",
      "Hyperparameters: {'weights': 'distance', 'p': 2, 'n_neighbors': 8}\n",
      "Mean Train MAE: 0.0000\n",
      "Mean Test MAE: 0.9474\n",
      "\n",
      "Hyperparameters: {'weights': 'uniform', 'p': 2, 'n_neighbors': 18}\n",
      "Mean Train MAE: 0.6116\n",
      "Mean Test MAE: 0.8949\n",
      "\n",
      "Hyperparameters: {'weights': 'uniform', 'p': 2, 'n_neighbors': 11}\n",
      "Mean Train MAE: 0.5340\n",
      "Mean Test MAE: 0.9247\n",
      "\n",
      "Hyperparameters: {'weights': 'uniform', 'p': 1, 'n_neighbors': 4}\n",
      "Mean Train MAE: 0.2995\n",
      "Mean Test MAE: 0.9616\n",
      "\n",
      "Hyperparameters: {'weights': 'distance', 'p': 2, 'n_neighbors': 4}\n",
      "Mean Train MAE: 0.0000\n",
      "Mean Test MAE: 1.0117\n",
      "\n",
      "Hyperparameters: {'weights': 'uniform', 'p': 2, 'n_neighbors': 29}\n",
      "Mean Train MAE: 0.6733\n",
      "Mean Test MAE: 0.8807\n",
      "\n",
      "Hyperparameters: {'weights': 'uniform', 'p': 1, 'n_neighbors': 20}\n",
      "Mean Train MAE: 0.5874\n",
      "Mean Test MAE: 0.8652\n",
      "\n",
      "Hyperparameters: {'weights': 'distance', 'p': 1, 'n_neighbors': 25}\n",
      "Mean Train MAE: -0.0000\n",
      "Mean Test MAE: 0.8596\n",
      "\n",
      "\n",
      "Best Hyperparameters: {'weights': 'uniform', 'p': 1, 'n_neighbors': 27}\n",
      "Best Average MAE on Validation Folds: 0.8553\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import mean_absolute_error, make_scorer\n",
    "import numpy as np\n",
    "\n",
    "# Load the data\n",
    "file_path = 'final_straddle_data_normalized_condensed.csv'\n",
    "df = pd.read_csv(file_path)\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "# Remove rows with dates in 2023 for the holdout set\n",
    "holdout_set = df[df['date'].dt.year == 2023]\n",
    "train_set = df[df['date'].dt.year != 2023]\n",
    "\n",
    "# Remove the month columns\n",
    "month_columns = [col for col in train_set.columns if 'month' in col.lower()]\n",
    "train_set = train_set.drop(columns=month_columns)\n",
    "holdout_set = holdout_set.drop(columns=month_columns)\n",
    "\n",
    "# Define features and target\n",
    "features_train = train_set.drop(columns=['date', 'delta_neutral_long_straddle_returns', 'mean_daily_delta_neutral_long_straddle_returns'])\n",
    "target_train = train_set['mean_daily_delta_neutral_long_straddle_returns']\n",
    "\n",
    "features_holdout = holdout_set.drop(columns=['date', 'delta_neutral_long_straddle_returns', 'mean_daily_delta_neutral_long_straddle_returns'])\n",
    "target_holdout = holdout_set['mean_daily_delta_neutral_long_straddle_returns']\n",
    "\n",
    "# Define the parameter grid for hyperparameter tuning\n",
    "param_dist = {\n",
    "    'n_neighbors': list(range(1, 31)),\n",
    "    'weights': ['uniform', 'distance'],\n",
    "    'p': [1, 2]  # p=1 for Manhattan distance, p=2 for Euclidean distance\n",
    "}\n",
    "\n",
    "# Define MAE as the scoring metric\n",
    "mae_scorer = make_scorer(mean_absolute_error, greater_is_better=False)\n",
    "\n",
    "# Initialize the KNN model\n",
    "knn = KNeighborsRegressor()\n",
    "\n",
    "# Initialize RandomizedSearchCV\n",
    "random_search = RandomizedSearchCV(estimator=knn, param_distributions=param_dist, n_iter=30, scoring=mae_scorer, cv=5, random_state=42, n_jobs=1, verbose=0, return_train_score=True)\n",
    "\n",
    "# Perform random search\n",
    "random_search.fit(features_train, target_train)\n",
    "\n",
    "# Print the average MAE on train folds and test folds for each hyperparameter set tested\n",
    "results = random_search.cv_results_\n",
    "for i in range(len(results['params'])):\n",
    "    print(f\"Hyperparameters: {results['params'][i]}\")\n",
    "    print(f\"Mean Train MAE: {-results['mean_train_score'][i]:.4f}\")\n",
    "    print(f\"Mean Test MAE: {-results['mean_test_score'][i]:.4f}\\n\")\n",
    "\n",
    "# Get the best parameters and the corresponding MAE\n",
    "best_params = random_search.best_params_\n",
    "best_score = random_search.best_score_\n",
    "\n",
    "print(f\"\\nBest Hyperparameters: {best_params}\")\n",
    "print(f\"Best Average MAE on Validation Folds: {-best_score:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "edcab39d-5d1c-4012-9309-27f7dbeedba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Holdout Set MAE: 0.7656\n"
     ]
    }
   ],
   "source": [
    "# Train the best model on the entire training set\n",
    "best_knn = random_search.best_estimator_\n",
    "best_knn.fit(features_train, target_train)\n",
    "\n",
    "# Predict on the holdout set\n",
    "y_holdout_pred = best_knn.predict(features_holdout)\n",
    "\n",
    "# Calculate and print the MAE on the holdout set\n",
    "holdout_mae = mean_absolute_error(target_holdout, y_holdout_pred)\n",
    "print(f\"Holdout Set MAE: {holdout_mae:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4f21da21-1f79-4822-a7fe-79bb761dfb4f",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The feature names should match those that were passed during fit.\nFeature names unseen at fit time:\n- predicted_mean_daily_delta_neutral_long_straddle_returns\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Predict on the holdout set\u001b[39;00m\n\u001b[1;32m      5\u001b[0m features_holdout \u001b[38;5;241m=\u001b[39m holdout_set\u001b[38;5;241m.\u001b[39mdrop(columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdate\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdelta_neutral_long_straddle_returns\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean_daily_delta_neutral_long_straddle_returns\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m----> 6\u001b[0m holdout_set[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpredicted_mean_daily_delta_neutral_long_straddle_returns\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mbest_knn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures_holdout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# Save the updated holdout set to a new CSV file\u001b[39;00m\n\u001b[1;32m      9\u001b[0m holdout_set\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfinal_straddle_data_raw_with_predictions_knn.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/neighbors/_regression.py:236\u001b[0m, in \u001b[0;36mKNeighborsRegressor.predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    220\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Predict the target for the provided data.\u001b[39;00m\n\u001b[1;32m    221\u001b[0m \n\u001b[1;32m    222\u001b[0m \u001b[38;5;124;03mParameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    231\u001b[0m \u001b[38;5;124;03m    Target values.\u001b[39;00m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    233\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mweights \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muniform\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m    234\u001b[0m     \u001b[38;5;66;03m# In that case, we do not need the distances to perform\u001b[39;00m\n\u001b[1;32m    235\u001b[0m     \u001b[38;5;66;03m# the weighting so we do not compute them.\u001b[39;00m\n\u001b[0;32m--> 236\u001b[0m     neigh_ind \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkneighbors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_distance\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    237\u001b[0m     neigh_dist \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/neighbors/_base.py:806\u001b[0m, in \u001b[0;36mKNeighborsMixin.kneighbors\u001b[0;34m(self, X, n_neighbors, return_distance)\u001b[0m\n\u001b[1;32m    804\u001b[0m         X \u001b[38;5;241m=\u001b[39m _check_precomputed(X)\n\u001b[1;32m    805\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 806\u001b[0m         X \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcsr\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mC\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    808\u001b[0m n_samples_fit \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_samples_fit_\n\u001b[1;32m    809\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_neighbors \u001b[38;5;241m>\u001b[39m n_samples_fit:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/base.py:548\u001b[0m, in \u001b[0;36mBaseEstimator._validate_data\u001b[0;34m(self, X, y, reset, validate_separately, **check_params)\u001b[0m\n\u001b[1;32m    483\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_validate_data\u001b[39m(\n\u001b[1;32m    484\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m    485\u001b[0m     X\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mno_validation\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    489\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mcheck_params,\n\u001b[1;32m    490\u001b[0m ):\n\u001b[1;32m    491\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Validate input data and set or check the `n_features_in_` attribute.\u001b[39;00m\n\u001b[1;32m    492\u001b[0m \n\u001b[1;32m    493\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    546\u001b[0m \u001b[38;5;124;03m        validated.\u001b[39;00m\n\u001b[1;32m    547\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 548\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_feature_names\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    550\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_tags()[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequires_y\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n\u001b[1;32m    551\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    552\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m estimator \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    553\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrequires y to be passed, but the target y is None.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    554\u001b[0m         )\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/sklearn/base.py:481\u001b[0m, in \u001b[0;36mBaseEstimator._check_feature_names\u001b[0;34m(self, X, reset)\u001b[0m\n\u001b[1;32m    476\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m missing_names \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m unexpected_names:\n\u001b[1;32m    477\u001b[0m     message \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    478\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFeature names must be in the same order as they were in fit.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    479\u001b[0m     )\n\u001b[0;32m--> 481\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(message)\n",
      "\u001b[0;31mValueError\u001b[0m: The feature names should match those that were passed during fit.\nFeature names unseen at fit time:\n- predicted_mean_daily_delta_neutral_long_straddle_returns\n"
     ]
    }
   ],
   "source": [
    "# Get the best model\n",
    "best_knn = random_search.best_estimator_\n",
    "\n",
    "# Predict on the holdout set\n",
    "features_holdout = holdout_set.drop(columns=['date', 'delta_neutral_long_straddle_returns', 'mean_daily_delta_neutral_long_straddle_returns'])\n",
    "holdout_set['predicted_mean_daily_delta_neutral_long_straddle_returns'] = best_knn.predict(features_holdout)\n",
    "\n",
    "# Save the updated holdout set to a new CSV file\n",
    "holdout_set.to_csv('final_straddle_data_raw_with_predictions_knn.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "843229d1-f867-484e-8beb-56b928cba0af",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
